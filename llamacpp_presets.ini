version = 1

; (Optional) This section provides global settings shared across all presets.
; If the same key is defined in a specific preset, it will override the value in this global section.
[*]
n-gpu-layers = -1
ctx-size = 0
no-mmap = false
prio = 3
no-context-shift = true
context-shift = 0
no-kv-offload = true
no-warmup = true
batch-size = 2048
ubatch-size = 512
flash-attn = true
jinja = true
threads = 8
log-verbosity = 3
no-slots = true
parallel = 1
; Set to -1 to ensure the GPU sleeps when not in use
sleep-idle-seconds = -1
; Use 8-bit KV Cache to save massive RAM for 96GB setup
cache-type-k = q8_0
cache-type-v = q8_0

[extra.gpt-oss-120b-GGUF]
model = /models/ggml-org/gpt-oss-120b-GGUF/gpt-oss-120b-mxfp4-00001-of-00003.gguf
jinja = true
n-predict = 262144
batch-size = 2048
ubatch-size = 512
repeat-penalty = 1.05
reasoning-format = none

[extra.claude-fast]
model = /models/unsloth/GLM-4.7-Flash/GLM-4.7-Flash-Q8_0.gguf
; GLM-4.7 requires high ctx for tool use
ctx-size = 0
; Recommended sampling for tool-calling/Claude Code
temp = 0.7
top-p = 1.0
min-p = 0.01
repeat-penalty = 1.0
batch-size = 2048
ubatch-size = 512
; Forces the model to skip the <think> tags which break tool parsing
jinja = true

[extra.claude-deep]
model = /models/unsloth/Llama-3.2-1B-Instruct/Llama-3.2-1B-Instruct-UD-Q4_K_XL.gguf
; Deep coding tasks often need more context
ctx-size = 0
temp = 0.6
top-p = 0.95
; Qwen 32B at Q8_0 is ~35GB + ~15GB KV Cache = 50GB
batch-size = 2048
ubatch-size = 2048
repeat-penalty = 1.0
jinja = true
